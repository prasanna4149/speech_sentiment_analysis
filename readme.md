# ğŸ™ï¸ Speech Emotion Recognition Web Application

![Python](https://img.shields.io/badge/Python-3.12-blue.svg)
![Django](https://img.shields.io/badge/Backend-Django-green)
![React](https://img.shields.io/badge/Frontend-React-blue)
![Accuracy](https://img.shields.io/badge/CNN%20Model-94%25%20Accuracy-yellowgreen)

A full-stack web application that classifies **human emotions** from uploaded **speech audio files** using Deep Learning.  
Built with **Django (Python Backend)** and **React.js (Frontend)** for a seamless, interactive experience.

---

## ğŸ§  Problem Statement

Speech carries deep emotional cues. Recognizing emotions from voice can empower:

- ğŸ¤– Virtual assistants to respond empathetically  
- ğŸ“ Customer support centers to assess client mood  
- ğŸ§‘â€âš•ï¸ Healthcare professionals to monitor mental well-being  
- ğŸ® Gaming & entertainment to create immersive experiences  

This project focuses on **offline emotion detection** from **pre-recorded audio files**, predicting emotions like **Angry**, **Happy**, **Sad**, etc.

---

## âœ¨ Features

- ğŸ¤ Upload an audio file (WAV format) to classify emotion
- ğŸ§  Deep Learning model trained for high accuracy
- ğŸŒ REST APIs built using Django
- ğŸ–¥ï¸ Interactive React.js frontend
- ğŸ§ª Multiple model architectures tested (LSTM, CNN, CLSTM)
- ğŸš€ Codebase ready for future real-time implementation

---

## ğŸš€ Tech Stack

| Layer        | Tools / Frameworks                      |
|--------------|------------------------------------------|
| **Frontend** | React.js, Axios, Material-UI             |
| **Backend**  | Django, Django REST Framework            |
| **ML/DL**    | TensorFlow, Keras, scikit-learn, librosa |
| **Database** | SQLite (for development)                 |
| **Others**   | Pickle, Jupyter Notebook, Git            |

---

## ğŸ—‚ï¸ Datasets

We used the **RAVDESS** dataset for training and testing.
- ğŸµ  **cermad(Crowd Sourced Emotional Multimodal Actors Dataset)**
    
     [Download cermad Dataset](https://www.kaggle.com/datasets/ejlok1/cremad)
- ğŸµ **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)**:  
  [Download RAVDESS Dataset](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)
 
- ğŸ™ï¸ **TESS**:  
  [Download TESS Dataset](https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess)

- ğŸ¤ **SAVEE**:  
  [Download SAVEE Dataset](https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee)




## ğŸ§ª Model Training Overview

We experimented with **three different Deep Learning models**:

| Model    | Description                   | Accuracy | Status               |
|----------|--------------------------------|----------|-----------------------|
| ğŸ§  LSTM   | Long Short-Term Memory         | ~85%     | Commented (lower perf.)|
| ğŸ§  CLSTM  | CNN + LSTM hybrid              | ~88%     | Commented (moderate perf.)|
| âœ… CNN    | Convolutional Neural Network   | **94%**  | âœ… Used in production |

- **CNN achieved the best accuracy (94%)** and was chosen for deployment.
- **Further improvements** can be made by increasing the number of epochs and fine-tuning the model.
- The other models (LSTM and CLSTM) are still present in the training code, but **commented out**.

---

## ğŸ“Š Results

- âœ… **Model Used**: CNN
- ğŸ¯ **Validation Accuracy**: 94%
- â³ **Potential**: Can be improved by:
  - Training for more epochs
  - Optimizing model hyperparameters
  - Expanding the training dataset

---

## ğŸ—ï¸ System Architecture

```mermaid
flowchart TD
    A[ğŸ¤ Upload Audio File] --> B[React Frontend]
    B --> C[Django REST API Backend]
    C --> D[Audio Preprocessing (MFCC Extraction)]
    D --> E[Deep Learning Model (CNN)]
    E --> F[Return Predicted Emotion]
    F --> G[Display Result on Frontend]

```

## ğŸ“‚ Project Structure
```
emotion_recognition/
â”œâ”€â”€ emotion-frontend/            # React frontend
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/
â”‚       â”œâ”€â”€ services/
â”‚       â””â”€â”€ App.js
â”œâ”€â”€ emotion_recognition/         # Django backend
â”‚   â”œâ”€â”€ emotion_app/             # Django app
â”‚   â”œâ”€â”€ media/                   # Uploaded audio files
â”‚   â”œâ”€â”€ models/                  # Saved models (.h5, .pkl)
â”‚   â”œâ”€â”€ predict/                 # Prediction logic (model loading, audio feature extraction)
â”‚   â”œâ”€â”€ manage.py
â”‚   â””â”€â”€ db.sqlite3
â”œâ”€â”€ datasets/                    # Datasets for training
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for model training
â”‚   â””â”€â”€ train_model.ipynb
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # ğŸ“„ You are here!
```

# âš™ï¸ How to Run Locally
1. Clone the repository
```bash
git clone https://github.com/yourusername/emotion_recognition.git
cd emotion_recognition
```
2. Backend Setup (Django)
```bash
cd emotion_recognition
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
python manage.py migrate
python manage.py runserver
```
The Django backend will run at http://localhost:8000

3. Frontend Setup (React)
```bash
cd emotion-frontend
npm install
npm start
```
The React app will be running at http://localhost:3000

# ğŸ“ˆ Model Training (Optional)
If you want to train the models yourself:

- Open `notebooks/train_model.ipynb`
- Choose model architecture (CNN / LSTM / CLSTM)
- Run all cells to train and export your model

# ğŸ“£ Future Enhancements
- ğŸ”¥ Live Real-Time Emotion Detection (Microphone Input)
- ğŸ“ˆ Deploying to cloud (AWS, Azure, GCP)
- ğŸ›¡ï¸ User Authentication & History Tracking
- ğŸ¨ UI/UX Improvements with Animations
- ğŸ“¡ WebSocket Integration for Real-time Prediction

# ğŸ™‹â€â™‚ï¸ Contributing
Pull requests are welcome.  
For major changes, please open an issue first to discuss what you would like to change.

Don't forget to leave a â­ if you like this project!

# ğŸ“ License
This project is open-source and available under the MIT License.

