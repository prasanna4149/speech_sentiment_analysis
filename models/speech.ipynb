{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f757031",
   "metadata": {},
   "source": [
    "### Load Model & Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d69b390b4f794e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T12:13:58.690580Z",
     "start_time": "2025-02-27T12:13:58.686147Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Apps\\Python\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 28 variables whereas the saved optimizer has 54 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All models and preprocessing objects loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import model_from_json, load_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import librosa\n",
    "\n",
    "# Load CNN model architecture\n",
    "with open(\"CNN_model.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load CNN model weights\n",
    "loaded_model.load_weights(\"CNN_model.weights.h5\")\n",
    "loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Load the fully trained model (if needed)\n",
    "trained_model = load_model(\"trained_model.h5\")\n",
    "\n",
    "# Load preprocessing objects\n",
    "with open(\"encoder2.pickle\", \"rb\") as f:\n",
    "    encoder2 = pickle.load(f)\n",
    "\n",
    "with open(\"scaler2.pickle\", \"rb\") as f:\n",
    "    scaler2 = pickle.load(f)\n",
    "\n",
    "print(\"✅ All models and preprocessing objects loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab15a80",
   "metadata": {},
   "source": [
    "### Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c268f9a02899f98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T12:13:59.074952Z",
     "start_time": "2025-02-27T12:13:59.070988Z"
    }
   },
   "outputs": [],
   "source": [
    "def zcr(data, frame_length, hop_length):\n",
    "    zcr = librosa.feature.zero_crossing_rate(data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "\n",
    "def rmse(data, frame_length=2048, hop_length=512):\n",
    "    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(rmse)\n",
    "\n",
    "def mfcc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sr)\n",
    "    return np.squeeze(mfcc.T) if not flatten else np.ravel(mfcc.T)\n",
    "\n",
    "def extract_features(data, sr=22050, frame_length=2048, hop_length=512, target_size=2376):\n",
    "    result = np.array([])\n",
    "\n",
    "    result = np.hstack((result,\n",
    "                        zcr(data, frame_length, hop_length),\n",
    "                        rmse(data, frame_length, hop_length),\n",
    "                        mfcc(data, sr, frame_length, hop_length)\n",
    "                       ))\n",
    "    \n",
    "    # Adjust feature vector size\n",
    "    if len(result) < target_size:\n",
    "        # Pad with zeros if smaller\n",
    "        result = np.pad(result, (0, target_size - len(result)), mode='constant')\n",
    "    else:\n",
    "        # Truncate if larger\n",
    "        result = result[:target_size]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_predict_feat(path):\n",
    "    d, s_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    res = extract_features(d)\n",
    "    result = np.array(res)\n",
    "    result = np.reshape(result, (1, 2376))  # Ensures correct shape\n",
    "    i_result = scaler2.transform(result)\n",
    "    final_result = np.expand_dims(i_result, axis=2)\n",
    "    \n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb6207",
   "metadata": {},
   "source": [
    "### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aff5844086732e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions1 = {1: 'Neutral', 2: 'Calm', 3: 'Happy', 4: 'Sad', 5: 'Angry', 6: 'Fear', 7: 'Disgust', 8: 'Surprise'}\n",
    "\n",
    "def prediction(path1):\n",
    "    res = get_predict_feat(path1)\n",
    "    predictions = loaded_model.predict(res)\n",
    "    y_pred = encoder2.inverse_transform(predictions)\n",
    "    print(f\"🎤 Predicted Emotion: {y_pred[0][0]}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09986df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step\n",
      "🎤 Predicted Emotion: neutral\n"
     ]
    }
   ],
   "source": [
    "prediction(\"Datasets/RAVDESS/Actor_02/03-01-01-01-01-01-02.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2505ca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "🎤 Predicted Emotion: happy\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "🎤 Predicted Emotion: angry\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "🎤 Predicted Emotion: disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "🎤 Predicted Emotion: fear\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "🎤 Predicted Emotion: neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "🎤 Predicted Emotion: sad\n"
     ]
    }
   ],
   "source": [
    "prediction(\"Datasets/CREMA -D/AudioWAV/1001_DFA_HAP_XX.wav\")\n",
    "prediction(\"Datasets/CREMA -D/AudioWAV/1001_DFA_ANG_XX.wav\")\n",
    "prediction(\"Datasets/CREMA -D/AudioWAV/1001_DFA_DIS_XX.wav\")\n",
    "prediction(\"Datasets/CREMA -D/AudioWAV/1001_DFA_FEA_XX.wav\")\n",
    "prediction(\"Datasets/CREMA -D/AudioWAV/1001_DFA_NEU_XX.wav\")\n",
    "prediction(\"Datasets/CREMA -D/AudioWAV/1001_DFA_SAD_XX.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "🎤 Predicted Emotion: angry\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "🎤 Predicted Emotion: surprise\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "🎤 Predicted Emotion: fear\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "🎤 Predicted Emotion: happy\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "🎤 Predicted Emotion: neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🎤 Predicted Emotion: surprise\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "🎤 Predicted Emotion: sad\n"
     ]
    }
   ],
   "source": [
    "prediction(\"Datasets/TESS Toronto emotional speech set data/OAF_angry/OAF_back_angry.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/OAF_disgust/OAF_back_disgust.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/OAF_Fear/OAF_back_fear.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/OAF_happy/OAF_back_happy.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/OAF_neutral/OA_bite_neutral.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/OAF_Pleasant_surprise/OAF_back_ps.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/OAF_Sad/OAF_back_sad.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2bdcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "🎤 Predicted Emotion: angry\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "🎤 Predicted Emotion: disgust\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "🎤 Predicted Emotion: fear\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "🎤 Predicted Emotion: happy\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "🎤 Predicted Emotion: neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "🎤 Predicted Emotion: surprise\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "🎤 Predicted Emotion: sad\n"
     ]
    }
   ],
   "source": [
    "prediction(\"Datasets/TESS Toronto emotional speech set data/YAF_angry/YAF_back_angry.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/YAF_disgust/YAF_back_disgust.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/YAF_fear/YAF_back_fear.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/YAF_happy/YAF_back_happy.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/YAF_neutral/YAF_back_neutral.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/YAF_pleasant_surprised/YAF_back_ps.wav\")\n",
    "prediction(\"Datasets/TESS Toronto emotional speech set data/YAF_sad/YAF_back_sad.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26e743e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🗣️ Detected Emotion: fear\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🗣️ Detected Emotion: disgust\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "🗣️ Detected Emotion: disgust\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "🗣️ Detected Emotion: disgust\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🗣️ Detected Emotion: fear\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "🗣️ Detected Emotion: angry\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "🗣️ Detected Emotion: disgust\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "🗣️ Detected Emotion: disgust\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "🗣️ Detected Emotion: disgust\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "🗣️ Detected Emotion: neutral\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "🗣️ Detected Emotion: neutral\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "🗣️ Detected Emotion: neutral\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "🗣️ Detected Emotion: neutral\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "🗣️ Detected Emotion: neutral\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "🗣️ Detected Emotion: fear\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🗣️ Detected Emotion: disgust\n",
      "🎤 Listening... Speak now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "🗣️ Detected Emotion: disgust\n",
      "🎤 Listening... Speak now!\n",
      "\n",
      "🛑 Real-time emotion recognition stopped.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import time\n",
    "\n",
    "# Function to record audio from microphone\n",
    "def record_audio(duration=2.5, sr=22050):\n",
    "    print(\"🎤 Listening... Speak now!\")\n",
    "    audio = sd.rec(int(duration * sr), samplerate=sr, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    return np.squeeze(audio)\n",
    "\n",
    "# Function to extract features from real-time audio\n",
    "def extract_features_live(data, sr=22050, frame_length=2048, hop_length=512, target_size=2376):\n",
    "    result = np.hstack((zcr(data, frame_length, hop_length),\n",
    "                         rmse(data, frame_length, hop_length),\n",
    "                         mfcc(data, sr, frame_length, hop_length)))\n",
    "    \n",
    "    # Adjust feature vector size\n",
    "    if len(result) < target_size:\n",
    "        result = np.pad(result, (0, target_size - len(result)), mode='constant')\n",
    "    else:\n",
    "        result = result[:target_size]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Real-time emotion recognition loop\n",
    "def live_emotion_recognition():\n",
    "    try:\n",
    "        while True:\n",
    "            audio_data = record_audio()\n",
    "            features = extract_features_live(audio_data)\n",
    "            features = np.reshape(features, (1, 2376))\n",
    "            features_scaled = scaler2.transform(features)\n",
    "            input_data = np.expand_dims(features_scaled, axis=2)\n",
    "\n",
    "            prediction = loaded_model.predict(input_data)\n",
    "            detected_emotion = encoder2.inverse_transform(prediction)\n",
    "\n",
    "            print(f\"🗣️ Detected Emotion: {detected_emotion[0][0]}\")\n",
    "            \n",
    "            time.sleep(0.5)  # Small delay before next prediction\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Real-time emotion recognition stopped.\")\n",
    "\n",
    "# Start real-time emotion detection\n",
    "live_emotion_recognition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Folder containing the audio files\n",
    "folder_path = r\"D:\\Library\\Documents\\Projects\\Speech Sentiment Analysis\\Datasets\\TESS Toronto emotional speech set data\\OAF_happy\"\n",
    "\n",
    "# Get all .wav files in the folder\n",
    "audio_files = glob.glob(os.path.join(folder_path, \"*.wav\"))\n",
    "\n",
    "# Loop through each file and predict\n",
    "for file in audio_files:\n",
    "    print(f\"🔍 Processing: {os.path.basename(file)}\")\n",
    "    prediction(file)\n",
    "    print(\"-\" * 50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641047c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
